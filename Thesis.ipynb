{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the jupyter notebook that is used for the experiments done in the bachelor thesis \"Clustering using an Entropy-Modified Gower Similarity Coefficient accounting for Categorical Variability\"."
      ],
      "metadata": {
        "id": "FUhwXkT11GxE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pu1Hob4su2F"
      },
      "source": [
        "Set seed for reproducability and import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDTOI7qT6zrE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.cluster import SpectralClustering, AgglomerativeClustering\n",
        "from collections import defaultdict\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from scipy.spatial.distance import squareform\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCMg1WtStoL-"
      },
      "source": [
        "#Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a short definition of the Gower similarity and the modification."
      ],
      "metadata": {
        "id": "vQwLZ73I2dfS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhZDb8Whtpws"
      },
      "source": [
        "First of all we look at the similarity measure proposed by Gower, 1971:      \\begin{align}\n",
        "\\mathbf{S}  = \\frac{\\sum_{k=1}^m s_{ijk}}{\\sum_{k=1}^m Î´_{ijk}}\n",
        "\\end{align}\n",
        "where $s_{ijk}$ has three different evaluation possibilities depending on the type of variable we deal with.\n",
        "If the variable is dichotomous (skip, just means it exists). If it is categorical we use:\n",
        "\\begin{align}\n",
        "s_{ijk} = \\begin{cases}\n",
        "             1  & \\text{if } x_{ik} == x_{jk} \\\\\n",
        "             0  & \\text{otherwise }\n",
        "       \\end{cases}\n",
        "\\end{align}\n",
        "If the variable is numerical we use:\n",
        "\\begin{align}\n",
        "s_{ijk} = 1 - \\frac{|x_i - x_j|}{R_k} \\quad \\text{,where } R_k \\text{ is the range of variable k}\n",
        "\\end{align}\n",
        " The following change to more accurately weigh categorical variables is proposed. A match in a variable with high variability should account for more similiarity than a match with a low variability one. I try to encorporate entropy to achieve that result. I will transform the $s_{ijk}$ for categorical variables in the following way:\n",
        "Denote by $U_k=\\{1,2,...,P\\}$ the amount of unique categories there exist for a given categorical variable $k$ and by $p_i$ the respective relative frequencises, then:\n",
        "\\begin{align}\n",
        "s_{ijk} = \\begin{cases}\n",
        "    \\frac{H(k)}{\\log_2{|U_k|}} & \\text{if } x_{ik} = x_{jk} \\\\\n",
        "    0 & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "For more discussion of this refer to Section 2.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5vOYZBotvCb"
      },
      "source": [
        "After doing a pairwise check and getting a similiarity matrix S of size (n_samples, n_samples), it is normalized, such that the similarity of a given object to itself is 1. Then the similarity matrix is transformed to a distance, using the formula of $D=1-S$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14mGOkaUugAs"
      },
      "source": [
        "Implementing it in code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0xFyU7_uiEz"
      },
      "source": [
        "##Defining Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gower package computes the Gower matrix for a given pandas Dataframe df, allowing for a weight vector w that assigns a weight to each feature. The package also needs a vector indicating which column is a categorical feature.\n",
        "\n",
        "So in order to compute the modified Gower matrix we have to compute the weights of the categorical features and specify which columns are categorical. The computation of the weights is done in the following code:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uDPPMpAT2tte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWeyrL3T6x8-"
      },
      "outputs": [],
      "source": [
        "def calculate_entropy(column): # Calculate entropy for a given column\n",
        "  if len(column)>0:\n",
        "    x=column.value_counts()\n",
        "    x=x/len(column)\n",
        "    ent=0\n",
        "    for i in range(len(x)):\n",
        "      ent-=x.iloc[i]*np.log2(x.iloc[i])\n",
        "    return ent\n",
        "\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def calculate_single_weight(column): # Calculate weight for a given column\n",
        "  return calculate_entropy(column)/np.log2(len(column))\n",
        "\n",
        "def calculate_weights(array,cat_vec): # Calculate the weights for all features, 1 if it numerical and the definition in Section 2.2 or in the text cell above for the modification\n",
        "  w=[]\n",
        "  for i in range(array.shape[1]):\n",
        "    if cat_vec[i]==True:\n",
        "      w.append(calculate_single_weight(array[array.columns[i]]))\n",
        "    else:\n",
        "      w.append(1)\n",
        "  return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TmEa7iY-y2K"
      },
      "outputs": [],
      "source": [
        "!pip install gower -qU # Make sure the gower package is installed\n",
        "import gower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-k6zPmhuyfn"
      },
      "source": [
        "#Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7laZTT0tnuaK"
      },
      "source": [
        "## Section 3.1: Creating a Mixed Synthetic Dataset\n",
        "\n",
        "The following function is used to create a **mixed synthetic dataset** by utilizing the `make_blobs` function from `sklearn`. This function creates data points for fixed hyperparameters.\n",
        "\n",
        "### Parameters:\n",
        "\n",
        "- **n_samples**: The number of data points to create.\n",
        "- **n_features**: The number of initial numerical features.\n",
        "- **centers**: The number of centers for clustering (similar to the concept of classes).\n",
        "- **cluster_std**: The amount of variance for a given center. This can be either:\n",
        "  - A list, where each element represents the variance for each center.\n",
        "  - A single number, representing the variance for all centers.\n",
        "- **n_bins**: The number of bins to divide each feature into using the `KBinsDiscretizer` function. This essentially splits the whole range of numbers into categories by assigning a category if a number belongs to a sub-interval.\n",
        "- **n_discrete**: Specifies the number of initial numerical features to discretize.\n",
        "- **n_uniform**: The number of features to add with a uniform distribution.\n",
        "- **n_categories**: The number of categories that the uniform distribution can pick from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oV1gDkX6m1q",
        "outputId": "109883e3-2e67-45b4-97a6-71395c2d71c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   feature_1  feature_2  feature_3  feature_4_disc  feature_5_disc  \\\n",
            "0  -9.989010   6.117292   9.527378             2.0             0.0   \n",
            "1  -3.076954  10.791599  10.099263             1.0             2.0   \n",
            "2  -2.503267  13.612906   0.604835             2.0             1.0   \n",
            "3  -6.208125  -6.628038   9.180104             2.0             3.0   \n",
            "4  -8.049167  10.140061   4.711128             1.0             1.0   \n",
            "\n",
            "  feature_6_unif feature_7_unif  label  \n",
            "0              C              C      0  \n",
            "1              D              C      2  \n",
            "2              A              C      0  \n",
            "3              C              C      1  \n",
            "4              C              D      2  \n"
          ]
        }
      ],
      "source": [
        "def create_mixed_synthetic_data(n_samples=5000, n_features=5, centers=4, cluster_std=3.0,\n",
        "                                n_bins=4, n_discrete=2, n_uniform=2, n_categories=4, random_state=42):\n",
        "    # Generate the dataset\n",
        "    data, labels = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers,\n",
        "                              cluster_std=cluster_std, random_state=random_state)\n",
        "\n",
        "    # Convert the numpy array to pandas dataframe\n",
        "    df = pd.DataFrame(data, columns=[f'feature_{i+1}' for i in range(n_features)])\n",
        "\n",
        "    # Discretize the last n_discrete columns into categorical variables\n",
        "    if n_discrete > 0:  # Check if n_discrete is greater than 0\n",
        "        kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
        "        df_discrete = kbd.fit_transform(df[df.columns[-n_discrete:]])\n",
        "\n",
        "        df_discrete = pd.DataFrame(df_discrete, columns=[f'feature_{i+1}_disc' for i in range(n_features - n_discrete, n_features)])\n",
        "        df = pd.concat([df[df.columns[:-n_discrete]], df_discrete], axis=1)\n",
        "\n",
        "    # For each categorical feature generated from uniform distribution\n",
        "    for i in range(n_features + 1, n_features + n_uniform + 1):\n",
        "        df[f'feature_{i}_unif'] = np.random.choice([chr(j) for j in range(65, 65 + n_categories)], size=n_samples)\n",
        "\n",
        "    # Adding the labels\n",
        "    df['label'] = labels\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example Dataframe\n",
        "df = create_mixed_synthetic_data()\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXsL9PQLr-bC"
      },
      "source": [
        "The `describe_data` function keeps a record of the range of the numerical features, the distribution of entries for categorical features and the number of bins that were used for discretization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4n3j3Ja-DcV"
      },
      "outputs": [],
      "source": [
        "def describe_data(df, n_discrete=2, n_uniform=2, n_bins=4):\n",
        "    # Determine the number of each type of columns\n",
        "    n_numerical = df.shape[1] - n_discrete - n_uniform - 1  # minus 1 for the label column\n",
        "\n",
        "    # Define numerical and categorical columns\n",
        "    num_columns = df.columns[:n_numerical]\n",
        "    cat_columns = df.columns[n_numerical:-1]  # Exclude the label column\n",
        "\n",
        "    # Initialize dictionary to store results\n",
        "    data_description = {}\n",
        "\n",
        "    # Record range for numerical columns\n",
        "    for col in num_columns:\n",
        "        data_description[col] = {\n",
        "            'min': df[col].min(),\n",
        "            'max': df[col].max()\n",
        "        }\n",
        "\n",
        "    # Record value counts for categorical columns\n",
        "    for col in cat_columns:\n",
        "        data_description[col] = {\n",
        "            'value_counts': df[col].value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "    return data_description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtcaow3vd5wW"
      },
      "source": [
        "#Section 3.5: Finding the best Linkage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3zVtNyOv4C-"
      },
      "source": [
        " The function for clustering given the computed Gower matrix and linkage parameter is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um0chdckvb2I"
      },
      "outputs": [],
      "source": [
        "def gower_ari(gow, df, centers=4,linkage_method='ward', criterion=\"maxclust\"):\n",
        "    # Extract the labels\n",
        "    labels = df['label']\n",
        "\n",
        "    # Perform hierarchical clustering\n",
        "    Zd = linkage(squareform(gow), method=linkage_method)\n",
        "\n",
        "    # Form flat clusters from the hierarchical clustering defined by the linkage matrix\n",
        "    if criterion==\"maxclust\":\n",
        "      cld1 = fcluster(Zd, centers, criterion=criterion)\n",
        "\n",
        "    # Compute Adjusted Rand index\n",
        "    ari = adjusted_rand_score(labels, cld1)\n",
        "\n",
        "    return ari, cld1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ffyAv9Qwi1R"
      },
      "source": [
        "Here the clustering result of the baseline using Agglomerative Clustering and the one-hot encoded version of the given dataframe is recorded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFa-4FMd9dKs"
      },
      "outputs": [],
      "source": [
        "def cluster_data_agg(df, centers=4, n_features=5, n_discrete=2, n_uniform=2):\n",
        "    # Extract the features and labels\n",
        "    X = df.drop(columns=['label'])\n",
        "    y = df['label']\n",
        "\n",
        "    # Calculate n_categorical from the sum of n_discrete and n_uniform\n",
        "    n_categorical = n_discrete + n_uniform\n",
        "\n",
        "    # Transform the data\n",
        "    transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), list(range(n_features-n_discrete, n_features + n_uniform)))],\n",
        "                                    remainder='passthrough', sparse_threshold=0)\n",
        "    X_transformed = transformer.fit_transform(X)\n",
        "\n",
        "    # Initialize and fit AgglomerativeClustering\n",
        "    agg = AgglomerativeClustering(n_clusters=centers)\n",
        "    agg.fit(X_transformed)\n",
        "\n",
        "    # Get cluster labels\n",
        "    cluster_labels = agg.labels_\n",
        "\n",
        "    # Compute Adjusted Rand index\n",
        "    ari = adjusted_rand_score(y, cluster_labels)\n",
        "\n",
        "    return ari, cluster_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lASyt8KdcIEa"
      },
      "source": [
        "## Function: `compare_clustering_methods`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7AxXPtZuUyH"
      },
      "source": [
        "This function evaluates various linkage methods on a given synthetic mixed dataset. It uses the Gower distance matrix and Agglomerative Clustering to compute the Adjusted Rand Index (ARI) for each parameter setting.\n",
        "\n",
        "### Parameters\n",
        "- **data**: The input dataset as a pandas DataFrame.\n",
        "- **n_features, n_discrete, n_uniform, centers, cluster_std, n_bins**: Parameters that may be used when generating synthetic data outside this function.\n",
        "\n",
        "### Function Process\n",
        "1. **Linkage Methods and Criteria**: Defines a list of linkage methods ('centroid', 'single', 'complete', 'average', 'ward') for hierarchical clustering.\n",
        "2. **Configurations**: Specifies configurations for the Gower method, focusing on weighting options (False, True).\n",
        "3. **Gower Distance Matrix**: Computes the Gower distance matrix once for each dataset and configuration.\n",
        "4. **ARI Calculation**: For each configuration, the function calculates the ARI for different linkage methods using the Gower distance matrix and stores them.\n",
        "6. **Result Compilation**: Merges ARI results from the Gower and Agglomerative methods.\n",
        "\n",
        "### Return\n",
        "- **results**: A dictionary containing ARI scores for the tested linkages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v48V1cZ0eGUq"
      },
      "outputs": [],
      "source": [
        "def compare_clustering_methods(data,n_features=5, n_discrete=2, n_uniform=2, centers=4, cluster_std=4,n_bins=4):\n",
        "    # Create mixed synthetic data\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        df = data\n",
        "\n",
        "    # Define the linkage methods and criteria to compare\n",
        "    linkage_methods = ['centroid', 'single', 'complete', 'average', 'ward']\n",
        "\n",
        "    # Define the weighting and scaling configurations for the Gower method\n",
        "    configs = [False,True]\n",
        "\n",
        "    # Extract the features\n",
        "    X = df.drop(columns=['label']).values\n",
        "\n",
        "    # Calculate n_categorical from the sum of n_discrete and n_uniform\n",
        "    n_categorical = n_discrete + n_uniform\n",
        "    cat_vec = [False] * (n_features - n_discrete) + [True] * n_categorical\n",
        "\n",
        "    # Calculate ARI for each combination of parameters using the Gower method\n",
        "    results_gower = {}\n",
        "    for config in configs:\n",
        "        weighting = config\n",
        "\n",
        "        # Compute the Gower distance matrix only once for each dataset\n",
        "        if weighting:\n",
        "            w = np.array(calculate_weights(df.drop(columns=['label']), cat_vec))\n",
        "            gow = gower.gower_matrix(X, cat_features=cat_vec, weight=w)\n",
        "        else:\n",
        "            gow = gower.gower_matrix(X, cat_features=cat_vec)\n",
        "\n",
        "        for linkage_method in linkage_methods:\n",
        "            ari, _ = gower_ari(gow, df, centers=centers, weighting=weighting, linkage_method=linkage_method)\n",
        "            results_gower[(linkage_method, weighting)] = ari\n",
        "\n",
        "\n",
        "    # Merge the results dictionaries and return\n",
        "    results = {**results_gower}\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMvDuLVU1ygF"
      },
      "source": [
        "## Function: compare_on_random_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT2mKTkGwYGM"
      },
      "source": [
        "\n",
        "\n",
        "This function evaluates the introduced linkage methods on multiple randomly generated datasets. It utilizes the previously described `create_mixed_synthetic_data` and `compare_clustering_methods` functions.\n",
        "\n",
        "### Parameters\n",
        "\n",
        "The parameter ranges are fixed according to Section 3.3.\n",
        "- **n_trials**: Number of random datasets to be generated for the evaluation.\n",
        "- **min_features, max_features**: Range for the random number of features to be used in the synthetic dataset.\n",
        "- **min_discrete, max_discrete**: Range for the random number of discrete features in the synthetic dataset.\n",
        "- **min_uniform, max_uniform**: Range for the random number of features with a uniform distribution in the synthetic dataset.\n",
        "- **min_centers, max_centers**: Range for the random number of centers for clustering in the synthetic dataset.\n",
        "- **cluster_std_range**: A tuple indicating the range for cluster standard deviation.\n",
        "\n",
        "### Internal Variables and Steps\n",
        "\n",
        "1. **total_ari_scores**: A defaultdict to hold the summed ARIs for each method.\n",
        "\n",
        "2. **dataset_descriptions**: A list to store the descriptions of each generated dataset.\n",
        "\n",
        "3. **individual_ari_scores**: A list to store ARI scores for individual datasets.\n",
        "\n",
        "4. A loop iterates `n_trials` times. Inside the loop:\n",
        "\n",
        "    a. Random parameters are generated within the specified ranges.\n",
        "    \n",
        "    b. A synthetic dataset is generated using the `create_mixed_synthetic_data` function.\n",
        "    \n",
        "    c. The description of the generated dataset is recorded.\n",
        "    \n",
        "    d. ARI scores are calculated using different clustering methods through the `compare_clustering_methods` function. The scores are updated in the `total_ari_scores` dictionary, and individual scores for this dataset are stored in `individual_dataset_ari_scores`.\n",
        "    \n",
        "    e. Individual scores for the dataset are appended to `individual_ari_scores`.\n",
        "\n",
        "5. Finally, average ARI scores for each method are calculated and stored in `avg_ari_scores`.\n",
        "\n",
        "### Return\n",
        "\n",
        "- **avg_ari_scores**: A dictionary containing the average ARI scores for different configurations and clustering methods.\n",
        "- **dataset_descriptions**: A list containing the descriptions of each generated dataset.\n",
        "- **individual_ari_scores**: A list containing dictionaries of individual ARI scores for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b1I85DSstR4"
      },
      "outputs": [],
      "source": [
        "def compare_on_random_datasets(n_trials=100, min_features=2, max_features=10,\n",
        "                               min_discrete=0, max_discrete=5,\n",
        "                               min_uniform=0, max_uniform=5,\n",
        "                               min_centers=2, max_centers=10,\n",
        "                               min_bins=2, max_bins=10,\n",
        "                               cluster_std_range=(1, 5)):\n",
        "\n",
        "    # Dictionary to hold the summed ARIs for each method and dataset descriptions\n",
        "    total_ari_scores = defaultdict(float)\n",
        "    dataset_descriptions = []\n",
        "    individual_ari_scores = []  # To store ARI scores for individual datasets\n",
        "\n",
        "    # Loop to generate different standard deviations and perform clustering comparison\n",
        "    for _ in range(n_trials):\n",
        "        # Generate random parameters\n",
        "        n_features = np.random.randint(min_features, max_features + 1)\n",
        "        n_discrete = np.random.randint(min_discrete, min(n_features, max_discrete + 1))\n",
        "        n_uniform = np.random.randint(min_uniform, max_uniform + 1)\n",
        "        centers = np.random.randint(min_centers, max_centers + 1)\n",
        "        cluster_std = np.random.uniform(cluster_std_range[0], cluster_std_range[1], centers)\n",
        "        n_bins=int(np.random.uniform(min_bins, max_bins+1))\n",
        "\n",
        "        # Generate the synthetic dataset\n",
        "        df = create_mixed_synthetic_data(n_samples=5000, n_features=n_features, centers=centers,\n",
        "                                         cluster_std=cluster_std, n_bins=n_bins,\n",
        "                                         n_discrete=n_discrete, n_uniform=n_uniform,\n",
        "                                         n_categories=4, random_state=42)\n",
        "\n",
        "        # Record the dataset description\n",
        "        description = describe_data(df, n_discrete=n_discrete, n_uniform=n_uniform)\n",
        "        description['std_devs'] = cluster_std.tolist()  # Add standard deviations to the description\n",
        "        dataset_descriptions.append(description)\n",
        "\n",
        "        # Initialize a dictionary to store the individual ARI scores for this dataset\n",
        "        individual_dataset_ari_scores = {}\n",
        "\n",
        "        # Get the ARI scores for the different clustering methods and update the total scores\n",
        "        ari_scores = compare_clustering_methods(df,n_features=n_features, n_discrete=n_discrete, n_uniform=n_uniform,\n",
        "                                                centers=centers, cluster_std=cluster_std, n_bins=4)\n",
        "\n",
        "        for method, ari in ari_scores.items():\n",
        "            total_ari_scores[method] += ari\n",
        "            individual_dataset_ari_scores[method] = ari\n",
        "\n",
        "        # Append the individual scores for this dataset to the main list\n",
        "        individual_ari_scores.append(individual_dataset_ari_scores)\n",
        "\n",
        "    # Divide the total scores by the number of trials to get the average score for each method\n",
        "    avg_ari_scores = {method: score / n_trials for method, score in total_ari_scores.items()}\n",
        "\n",
        "    return avg_ari_scores, dataset_descriptions, individual_ari_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frID6wMNt9OX"
      },
      "source": [
        "In this code cell, the modified `compare_on_random_datasets` function is called with 100 trials (`n_trials=100`). This function generates 100 random synthetic datasets and evaluates different configurations of clustering methods on them. The average Adjusted Rand Index (ARI) scores across all the datasets are stored in the variable `average`. The results are reported in Section 3.5.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5ieSW1AYWqCg"
      },
      "outputs": [],
      "source": [
        "max_features=15\n",
        "average, description, ind=compare_on_random_datasets(n_trials=100,max_features=max_features,max_discrete=max_features, max_uniform=8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qygweZAYmmeE",
        "outputId": "175f0748-8efc-455b-eb87-36acaf30f697"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('centroid', False): 0.05922121056941135,\n",
              " ('single', False): 0.008368815370272658,\n",
              " ('complete', False): 0.3210377249774347,\n",
              " ('average', False): 0.4258865857256059,\n",
              " ('ward', False): 0.509187433502024,\n",
              " ('centroid', True): 0.20252709274608247,\n",
              " ('single', True): 0.010770037707485709,\n",
              " ('complete', True): 0.5937625259463986,\n",
              " ('average', True): 0.5536079550074489,\n",
              " ('ward', True): 0.7942905005032825}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 3.7"
      ],
      "metadata": {
        "id": "byloKesvv8ZB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv2CiWTm1ESu"
      },
      "source": [
        "## Modifications to the `compare_on_random_datasets` Function\n",
        "\n",
        "The code cell contains an adapted version of the `compare_on_random_datasets` function. The main structure of the function remains intact, focused on generating synthetic datasets and evaluating the introduced clustering methods in Section 3. Here are the notable differences from the original version:\n",
        "\n",
        "### Focused Configuration Testing\n",
        "The function now specifically targets a subset of configurations for the Gower method. It narrows down the testing scope to configurations using the `ward` linkage method. This is achieved by defining the `gower_configs` list with the configurations of interest.\n",
        "\n",
        "### Individual Score Recording\n",
        "The function now captures individual Adjusted Rand Index (ARI) scores for each dataset and configuration. These are stored in a dictionary named `individual_dataset_ari_scores`, which is subsequently appended to a list called `individual_ari_scores` after each trial.\n",
        "\n",
        "### HDBSCAN Implementation\n",
        "The function includes the unsupervised HDBSCAN clustering algorithm, applied both directly to the dataset and using a Gower matrix as a distance metric.\n",
        "\n",
        "### Spectral Clustering\n",
        "Spectral clustering is also added to the set of methods for evaluation, employing the Gower matrix for calculation.\n",
        "\n",
        "### Additional Baseline Clustering\n",
        "The function now includes a baseline using Agglomerative Clustering. This provides another reference point for evaluating clustering methods.\n",
        "\n",
        "### Enhanced Output\n",
        "The function continues to return the average ARI scores, dataset descriptions, and individual ARI scores, providing a detailed evaluation of each clustering method's performance on synthetic datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUZu0C75Gdn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f6970a-8c41-4433-911e-2854933db907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m0.0/5.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ¸\u001b[0m\u001b[90mâââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m0.1/5.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K     \u001b[91mâââââââââââââââââââââââ\u001b[0m\u001b[91mâ¸\u001b[0m\u001b[90mââââââââââââââââ\u001b[0m \u001b[32m3.1/5.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâââââââââââââââââââââââââââââââââââââââ\u001b[0m\u001b[91mâ¸\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install hdbscan -qU # Make sure hdbscan library is installed\n",
        "import hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mdScaMtU2IM"
      },
      "outputs": [],
      "source": [
        "def spectral_clustering2(gow=None, df=None, centers=4): # Perform Spectral Clustering\n",
        "  clustering = SpectralClustering(n_clusters=centers, affinity='precomputed').fit(gow)\n",
        "  ari = adjusted_rand_score(clustering.labels_, df[\"label\"])\n",
        "  return ari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v01AObu1hxqo"
      },
      "outputs": [],
      "source": [
        "def compare_on_random_datasets(n_trials=100, min_features=2, max_features=10,\n",
        "                               min_discrete=0, max_discrete=5,\n",
        "                               min_uniform=0, max_uniform=5,\n",
        "                               min_centers=2, max_centers=10,\n",
        "                               min_bins=2, max_bins=10,\n",
        "                               cluster_std_range=(1, 5)):\n",
        "\n",
        "    total_ari_scores = defaultdict(float)\n",
        "    dataset_descriptions = []\n",
        "    individual_ari_scores = []\n",
        "\n",
        "\n",
        "    for _ in range(n_trials):\n",
        "        if _ % 10 == 0: # Keep track of how far the program is since it takes a while\n",
        "            print(_)\n",
        "\n",
        "        # Existing random dataset generation code\n",
        "        n_features = np.random.randint(min_features, max_features + 1)\n",
        "        n_discrete = np.random.randint(min_discrete, min(n_features, max_discrete + 1))\n",
        "        n_uniform = np.random.randint(min_uniform, max_uniform + 1)\n",
        "        centers = np.random.randint(min_centers, max_centers + 1)\n",
        "        cluster_std = np.random.uniform(cluster_std_range[0], cluster_std_range[1], centers)\n",
        "        n_bins=int(np.random.uniform(min_bins, max_bins+1))\n",
        "\n",
        "        df = create_mixed_synthetic_data(n_samples=5000, n_features=n_features, centers=centers,\n",
        "                                         cluster_std=cluster_std, n_bins=n_bins,\n",
        "                                         n_discrete=n_discrete, n_uniform=n_uniform,\n",
        "                                         n_categories=4, random_state=42)\n",
        "\n",
        "        description = describe_data(df, n_discrete=n_discrete, n_uniform=n_uniform, n_bins=n_bins)\n",
        "        description['std_devs'] = cluster_std.tolist()\n",
        "        dataset_descriptions.append(description)\n",
        "\n",
        "        individual_dataset_ari_scores = {}\n",
        "        X = df.drop(columns=['label']).values\n",
        "\n",
        "        # Calculating n_categorical and cat_vec as in original code\n",
        "        n_categorical = n_discrete + n_uniform\n",
        "        cat_vec = [False] * (n_features - n_discrete) + [True] * n_categorical\n",
        "\n",
        "        gower_configs = [('ward', 'maxclust', False),\n",
        "                         ('ward', 'maxclust', True)]\n",
        "\n",
        "\n",
        "        gower_matrices = {}\n",
        "        for config in gower_configs:\n",
        "            if config[2] not in gower_matrices:\n",
        "                # Compute Gower matrix\n",
        "                if config[2]:\n",
        "                    w = np.array(calculate_weights(df.drop(columns=['label']), cat_vec))\n",
        "                    gower_matrices[(config[2])] = gower.gower_matrix(X, cat_features=cat_vec, weight=w)\n",
        "                else:\n",
        "                    gower_matrices[(config[2])] = gower.gower_matrix(X, cat_features=cat_vec)\n",
        "\n",
        "            gow = gower_matrices[(config[2])]\n",
        "\n",
        "            # HAC with Ward\n",
        "            ari, _ = gower_ari(gow, df,centers=centers, linkage_method=config[0])\n",
        "            method_name = f'gower_{config[0]}_{config[1]}_weighting={config[2]}'\n",
        "            total_ari_scores[method_name] += ari\n",
        "            individual_dataset_ari_scores[method_name] = ari\n",
        "\n",
        "            # Spectral Clustering\n",
        "            sim=1-gow      #transform back to similarity matrix for spectral clustering\n",
        "            ari=spectral_clustering2(gow=sim, df=df, centers=centers) #perform spectral clustering\n",
        "            method_name = f'spectral_weighting={config[2]}'\n",
        "            total_ari_scores[method_name] += ari\n",
        "\n",
        "            individual_dataset_ari_scores[method_name] = ari\n",
        "            # Perform HDBSCAN with Gower matrix\n",
        "            gow_double = gow.astype(np.float64)\n",
        "            hdbscan_cluster = hdbscan.HDBSCAN(metric='precomputed')\n",
        "            labels = hdbscan_cluster.fit_predict(gow_double)\n",
        "            ari = adjusted_rand_score(df['label'], labels)\n",
        "            method_name_hdbscan = f'HDBSCAN_weighting={config[2]}'\n",
        "            total_ari_scores[method_name_hdbscan] += ari\n",
        "            individual_dataset_ari_scores[method_name_hdbscan] = ari\n",
        "        # HDBSCAN baseline\n",
        "        if any(cat_vec):\n",
        "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "            X_categorical = encoder.fit_transform(X[:, np.array(cat_vec)])\n",
        "            X_numerical = X[:, ~np.array(cat_vec)]\n",
        "            X_combined = np.hstack([X_numerical, X_categorical])\n",
        "        else:\n",
        "            X_combined = X  # Use the original dataset if no categorical variables\n",
        "\n",
        "        hdbscan_cluster = hdbscan.HDBSCAN()\n",
        "        labels = hdbscan_cluster.fit_predict(X_combined)\n",
        "        ari = adjusted_rand_score(df['label'], labels)\n",
        "        method_name_hdbscan_direct = 'HDBSCAN_baseline'\n",
        "        total_ari_scores[method_name_hdbscan_direct] += ari\n",
        "        individual_dataset_ari_scores[method_name_hdbscan_direct] = ari\n",
        "        # Agglomerative Clustering baseline\n",
        "        ari, _ = cluster_data_agg(df, centers=centers,\n",
        "                                  n_features=n_features, n_discrete=n_discrete,\n",
        "                                  n_uniform=n_uniform)\n",
        "        total_ari_scores['AgglomerativeClustering_baseline'] += ari\n",
        "        individual_dataset_ari_scores['AgglomerativeClustering_baseline'] = ari\n",
        "\n",
        "        individual_ari_scores.append(individual_dataset_ari_scores)\n",
        "\n",
        "    avg_ari_scores = {method: score / n_trials for method, score in total_ari_scores.items()}\n",
        "\n",
        "    return avg_ari_scores, dataset_descriptions, individual_ari_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell runs the function for 200 trials:"
      ],
      "metadata": {
        "id": "ReNG20QbakLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41z-miB4iR97"
      },
      "outputs": [],
      "source": [
        "max_features=15\n",
        "average,description,individual=compare_on_random_datasets(n_trials=1, max_features=max_features,max_discrete=max_features, max_uniform=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mYWqyUJVmS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f440eab-132f-4387-986d-b81b2a6af61c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gower_ward_maxclust_weighting=False': 0.5459322209873017,\n",
              " 'spectral_weighting=False': 0.5106786723179865,\n",
              " 'HDBSCAN_weighting=False': 0.1804563465755983,\n",
              " 'gower_ward_maxclust_weighting=True': 0.7331035637501517,\n",
              " 'spectral_weighting=True': 0.7121260862011384,\n",
              " 'HDBSCAN_weighting=True': 0.4190600636963385,\n",
              " 'AgglomerativeClustering_baseline': 0.7336948915629271,\n",
              " 'HDBSCAN_baseline': 0.4306307965240029}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "average # Average ARI of each clustering method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6-j4cwlNtkU"
      },
      "outputs": [],
      "source": [
        "# When is Spectral Clustering better with original Gower similarity matrix than with modification\n",
        "indices_spectral = [i for i in range(len(individual))\n",
        "                  if individual[i]['spectral_weighting=False'] >\n",
        "                  individual[i]['spectral_weighting=True']]\n",
        "# When is HAC with original Gower distance matrix better than modification\n",
        "indices_case_gower = [i for i in range(len(individual))\n",
        "                  if individual[i]['gower_ward_maxclust_weighting=False'] >\n",
        "                  individual[i]['gower_ward_maxclust_weighting=True']]\n",
        "# When is HDBSCAN better with original Gower distance matrix than with modification\n",
        "indices_hdbscan = [i for i in range(len(individual))\n",
        "                  if individual[i]['HDBSCAN_weighting=False'] >\n",
        "                  individual[i]['HDBSCAN_weighting=True']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGoru57X0JVW"
      },
      "source": [
        "## Function: `compute_mean_params`\n",
        "\n",
        "This function computes the mean attributes of dataset descriptions and performance scores for various algorithms. It processes the mean count of different feature types (discrete, uniform, numerical) along with their standard deviations. It also calculates mean performance scores for specific clustering algorithms. It is used to find cases, where clustering using the original Gower matrix outperformed clustering using the modified one.\n",
        "\n",
        "### Parameters\n",
        "- **indices**: A list of indices representing specific datasets.\n",
        "- **description_list**: A list containing dictionaries that describe features of datasets.\n",
        "- **individual_list**: A list of dictionaries detailing performance scores for different clustering algorithms.\n",
        "\n",
        "### Function Process\n",
        "1. Initializes variables for feature counts, standard deviations, and performance scores.\n",
        "2. Iterates through dataset indices to accumulate feature counts, standard deviations, and algorithm scores.\n",
        "3. Calculates mean and percentage-based statistics.\n",
        "4. Stores all results in a Pandas dataframe for easy manipulation.\n",
        "\n",
        "### Return\n",
        "A Pandas dataframe containing the following columns:\n",
        "- **num_indices**: The number of indices provided.\n",
        "- **mean_num_features**: The mean number of total features.\n",
        "- **mean_discrete_features**: The mean number of discrete features.\n",
        "- **mean_uniform_features**: The mean number of uniform features.\n",
        "- **mean_std_dev**: The mean standard deviation across all features.\n",
        "- **mean_max_std_dev**: The mean of the maximum standard deviations across all features.\n",
        "- **mean_min_std_dev**: The mean of the minimum standard deviations across all features.\n",
        "- **percentage_categorical_features**: The mean percentage of categorical (discrete and uniform) features.\n",
        "- **percentage_numerical_features**: The mean percentage of numerical features.\n",
        "- **mean_[algorithm_key]**: The mean performance score for each algorithm, where `algorithm_key` is dynamically identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eeBeC39Zz8-"
      },
      "outputs": [],
      "source": [
        "def compute_mean_params(indices, description_list, individual_list):\n",
        "    # Variables to store the various counts\n",
        "    std_dev_means = []\n",
        "    std_dev_maxs = []\n",
        "    std_dev_mins = []\n",
        "    num_discrete_features = 0\n",
        "    num_uniform_features = 0\n",
        "    num_num_features = 0\n",
        "\n",
        "    # Identify all keys dynamically from the first 'individual'\n",
        "    if individual_list:\n",
        "        all_keys = list(individual_list[0].keys())\n",
        "    else:\n",
        "        all_keys = []\n",
        "\n",
        "    scores = {key: [] for key in all_keys}\n",
        "\n",
        "    for i in indices:\n",
        "        desc = description_list[i]\n",
        "        individual = individual_list[i]\n",
        "        std_dev_current = desc['std_devs']\n",
        "\n",
        "        std_dev_means.append(np.mean(std_dev_current) if std_dev_current else 0)\n",
        "        std_dev_maxs.append(np.max(std_dev_current) if std_dev_current else 0)\n",
        "        std_dev_mins.append(np.min(std_dev_current) if std_dev_current else 0)\n",
        "\n",
        "        num_discrete_features += len([key for key in desc if '_disc' in key])\n",
        "        num_uniform_features += len([key for key in desc if '_unif' in key])\n",
        "        num_num_features += len([key for key in desc if '_disc' not in key and '_unif' not in key and key != 'std_devs'])\n",
        "\n",
        "        for key in all_keys:\n",
        "            scores[key].append(individual[key])\n",
        "\n",
        "    mean_std_dev = np.mean(std_dev_means) if std_dev_means else 0\n",
        "    mean_max_std_dev = np.mean(std_dev_maxs) if std_dev_maxs else 0\n",
        "    mean_min_std_dev = np.mean(std_dev_mins) if std_dev_mins else 0\n",
        "    mean_discrete_features = num_discrete_features / len(indices)\n",
        "    mean_uniform_features = num_uniform_features / len(indices)\n",
        "    mean_num_features = num_num_features / len(indices)\n",
        "\n",
        "    total_features = num_discrete_features + num_uniform_features + num_num_features\n",
        "    percentage_categorical_features = ((num_discrete_features + num_uniform_features) / total_features) * 100\n",
        "    percentage_numerical_features = (num_num_features / total_features) * 100\n",
        "\n",
        "    mean_scores = {key: np.mean(score) if score else 0 for key, score in scores.items()}\n",
        "\n",
        "    data = {\n",
        "        'num_indices': [len(indices)],\n",
        "        'mean_num_features': [mean_num_features],\n",
        "        'mean_discrete_features': [mean_discrete_features],\n",
        "        'mean_uniform_features': [mean_uniform_features],\n",
        "        'mean_std_dev': [mean_std_dev],\n",
        "        'mean_max_std_dev': [mean_max_std_dev],\n",
        "        'mean_min_std_dev': [mean_min_std_dev],\n",
        "        'percentage_categorical_features': [percentage_categorical_features],\n",
        "        'percentage_numerical_features': [percentage_numerical_features],\n",
        "        **{f\"mean_{key}\": [mean_score] for key, mean_score in mean_scores.items()}\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "xT5iQJGgxSA4",
        "outputId": "e06c4e93-62dc-49cf-f222-56ba6984e3b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   num_indices  mean_num_features  mean_discrete_features  \\\n",
              "0          200               4.98                     3.9   \n",
              "\n",
              "   mean_uniform_features  mean_std_dev  mean_max_std_dev  mean_min_std_dev  \\\n",
              "0                  4.155      3.010854          4.333296          1.673985   \n",
              "\n",
              "   percentage_categorical_features  percentage_numerical_features  \\\n",
              "0                        61.795167                      38.204833   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=False_scaling=False  \\\n",
              "0                                           0.545932        \n",
              "\n",
              "   mean_spectral_weighting=False_scaling=False  \\\n",
              "0                                     0.510679   \n",
              "\n",
              "   mean_HDBSCAN_weighting=False_scaling=False  \\\n",
              "0                                    0.180456   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=True_scaling=False  \\\n",
              "0                                           0.819667       \n",
              "\n",
              "   mean_spectral_weighting=True_scaling=False  \\\n",
              "0                                     0.78097   \n",
              "\n",
              "   mean_HDBSCAN_weighting=True_scaling=False  \\\n",
              "0                                   0.389445   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=True_scaling=True  \\\n",
              "0                                           0.733104      \n",
              "\n",
              "   mean_spectral_weighting=True_scaling=True  \\\n",
              "0                                   0.712126   \n",
              "\n",
              "   mean_HDBSCAN_weighting=True_scaling=True  mean_AgglomerativeClustering  \n",
              "0                                   0.41906                      0.733695  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e43cc16a-3c15-4e5c-b175-d1c851f8084d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_indices</th>\n",
              "      <th>mean_num_features</th>\n",
              "      <th>mean_discrete_features</th>\n",
              "      <th>mean_uniform_features</th>\n",
              "      <th>mean_std_dev</th>\n",
              "      <th>mean_max_std_dev</th>\n",
              "      <th>mean_min_std_dev</th>\n",
              "      <th>percentage_categorical_features</th>\n",
              "      <th>percentage_numerical_features</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=False_scaling=False</th>\n",
              "      <th>mean_spectral_weighting=False_scaling=False</th>\n",
              "      <th>mean_HDBSCAN_weighting=False_scaling=False</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=True_scaling=False</th>\n",
              "      <th>mean_spectral_weighting=True_scaling=False</th>\n",
              "      <th>mean_HDBSCAN_weighting=True_scaling=False</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=True_scaling=True</th>\n",
              "      <th>mean_spectral_weighting=True_scaling=True</th>\n",
              "      <th>mean_HDBSCAN_weighting=True_scaling=True</th>\n",
              "      <th>mean_AgglomerativeClustering</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200</td>\n",
              "      <td>4.98</td>\n",
              "      <td>3.9</td>\n",
              "      <td>4.155</td>\n",
              "      <td>3.010854</td>\n",
              "      <td>4.333296</td>\n",
              "      <td>1.673985</td>\n",
              "      <td>61.795167</td>\n",
              "      <td>38.204833</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.510679</td>\n",
              "      <td>0.180456</td>\n",
              "      <td>0.819667</td>\n",
              "      <td>0.78097</td>\n",
              "      <td>0.389445</td>\n",
              "      <td>0.733104</td>\n",
              "      <td>0.712126</td>\n",
              "      <td>0.41906</td>\n",
              "      <td>0.733695</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e43cc16a-3c15-4e5c-b175-d1c851f8084d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e43cc16a-3c15-4e5c-b175-d1c851f8084d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e43cc16a-3c15-4e5c-b175-d1c851f8084d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "compute_mean_params([x for x in range(len(individual))], description,individual) # Average parameters over all 200 trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "4d4327b7-ca60-4f32-80c6-07c028cff769",
        "id": "5kI3OrgIgq-4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   num_indices  mean_num_features  mean_discrete_features  \\\n",
              "0            4                1.0                    11.5   \n",
              "\n",
              "   mean_uniform_features  mean_std_dev  mean_max_std_dev  mean_min_std_dev  \\\n",
              "0                   3.25      3.270862          4.395287          1.966623   \n",
              "\n",
              "   percentage_categorical_features  percentage_numerical_features  \\\n",
              "0                        93.650794                       6.349206   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=False_scaling=False  \\\n",
              "0                                           0.894633        \n",
              "\n",
              "   mean_spectral_weighting=False_scaling=False  \\\n",
              "0                                     0.801649   \n",
              "\n",
              "   mean_HDBSCAN_weighting=False_scaling=False  \\\n",
              "0                                     0.27709   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=True_scaling=False  \\\n",
              "0                                           0.869301       \n",
              "\n",
              "   mean_spectral_weighting=True_scaling=False  \\\n",
              "0                                    0.848563   \n",
              "\n",
              "   mean_HDBSCAN_weighting=True_scaling=False  \\\n",
              "0                                   0.329144   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=True_scaling=True  \\\n",
              "0                                           0.177913      \n",
              "\n",
              "   mean_spectral_weighting=True_scaling=True  \\\n",
              "0                                   0.280476   \n",
              "\n",
              "   mean_HDBSCAN_weighting=True_scaling=True  mean_AgglomerativeClustering  \n",
              "0                                  0.226118                      0.238455  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9b6f154-9ba7-40f8-9c59-e157854a2fb9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_indices</th>\n",
              "      <th>mean_num_features</th>\n",
              "      <th>mean_discrete_features</th>\n",
              "      <th>mean_uniform_features</th>\n",
              "      <th>mean_std_dev</th>\n",
              "      <th>mean_max_std_dev</th>\n",
              "      <th>mean_min_std_dev</th>\n",
              "      <th>percentage_categorical_features</th>\n",
              "      <th>percentage_numerical_features</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=False_scaling=False</th>\n",
              "      <th>mean_spectral_weighting=False_scaling=False</th>\n",
              "      <th>mean_HDBSCAN_weighting=False_scaling=False</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=True_scaling=False</th>\n",
              "      <th>mean_spectral_weighting=True_scaling=False</th>\n",
              "      <th>mean_HDBSCAN_weighting=True_scaling=False</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=True_scaling=True</th>\n",
              "      <th>mean_spectral_weighting=True_scaling=True</th>\n",
              "      <th>mean_HDBSCAN_weighting=True_scaling=True</th>\n",
              "      <th>mean_AgglomerativeClustering</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>3.25</td>\n",
              "      <td>3.270862</td>\n",
              "      <td>4.395287</td>\n",
              "      <td>1.966623</td>\n",
              "      <td>93.650794</td>\n",
              "      <td>6.349206</td>\n",
              "      <td>0.894633</td>\n",
              "      <td>0.801649</td>\n",
              "      <td>0.27709</td>\n",
              "      <td>0.869301</td>\n",
              "      <td>0.848563</td>\n",
              "      <td>0.329144</td>\n",
              "      <td>0.177913</td>\n",
              "      <td>0.280476</td>\n",
              "      <td>0.226118</td>\n",
              "      <td>0.238455</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9b6f154-9ba7-40f8-9c59-e157854a2fb9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d9b6f154-9ba7-40f8-9c59-e157854a2fb9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d9b6f154-9ba7-40f8-9c59-e157854a2fb9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "compute_mean_params(indices_case_gower, description, individual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "b454f670-6535-402b-b12b-95e1edddfecf",
        "id": "a1F3dsS0gq-5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   num_indices  mean_num_features  mean_discrete_features  \\\n",
              "0           12                2.5                    5.75   \n",
              "\n",
              "   mean_uniform_features  mean_std_dev  mean_max_std_dev  mean_min_std_dev  \\\n",
              "0                    4.0      3.075745          4.123308          1.965802   \n",
              "\n",
              "   percentage_categorical_features  percentage_numerical_features  \\\n",
              "0                        79.591837                      20.408163   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=False_scaling=False  \\\n",
              "0                                           0.721258        \n",
              "\n",
              "   mean_spectral_weighting=False_scaling=False  \\\n",
              "0                                     0.795671   \n",
              "\n",
              "   mean_HDBSCAN_weighting=False_scaling=False  \\\n",
              "0                                    0.296839   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=True_scaling=False  \\\n",
              "0                                           0.843252       \n",
              "\n",
              "   mean_spectral_weighting=True_scaling=False  \\\n",
              "0                                    0.763663   \n",
              "\n",
              "   mean_HDBSCAN_weighting=True_scaling=False  \\\n",
              "0                                   0.400606   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=True_scaling=True  \\\n",
              "0                                           0.585921      \n",
              "\n",
              "   mean_spectral_weighting=True_scaling=True  \\\n",
              "0                                   0.620761   \n",
              "\n",
              "   mean_HDBSCAN_weighting=True_scaling=True  mean_AgglomerativeClustering  \n",
              "0                                   0.40599                      0.588933  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31e764f5-ab48-4016-8f24-f032e7220f60\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_indices</th>\n",
              "      <th>mean_num_features</th>\n",
              "      <th>mean_discrete_features</th>\n",
              "      <th>mean_uniform_features</th>\n",
              "      <th>mean_std_dev</th>\n",
              "      <th>mean_max_std_dev</th>\n",
              "      <th>mean_min_std_dev</th>\n",
              "      <th>percentage_categorical_features</th>\n",
              "      <th>percentage_numerical_features</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=False_scaling=False</th>\n",
              "      <th>mean_spectral_weighting=False_scaling=False</th>\n",
              "      <th>mean_HDBSCAN_weighting=False_scaling=False</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=True_scaling=False</th>\n",
              "      <th>mean_spectral_weighting=True_scaling=False</th>\n",
              "      <th>mean_HDBSCAN_weighting=True_scaling=False</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=True_scaling=True</th>\n",
              "      <th>mean_spectral_weighting=True_scaling=True</th>\n",
              "      <th>mean_HDBSCAN_weighting=True_scaling=True</th>\n",
              "      <th>mean_AgglomerativeClustering</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.75</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.075745</td>\n",
              "      <td>4.123308</td>\n",
              "      <td>1.965802</td>\n",
              "      <td>79.591837</td>\n",
              "      <td>20.408163</td>\n",
              "      <td>0.721258</td>\n",
              "      <td>0.795671</td>\n",
              "      <td>0.296839</td>\n",
              "      <td>0.843252</td>\n",
              "      <td>0.763663</td>\n",
              "      <td>0.400606</td>\n",
              "      <td>0.585921</td>\n",
              "      <td>0.620761</td>\n",
              "      <td>0.40599</td>\n",
              "      <td>0.588933</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31e764f5-ab48-4016-8f24-f032e7220f60')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-31e764f5-ab48-4016-8f24-f032e7220f60 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-31e764f5-ab48-4016-8f24-f032e7220f60');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "compute_mean_params(indices_spectral, description, individual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_mean_params(indices_hdbscan, description, individual)"
      ],
      "metadata": {
        "outputId": "634fd329-7756-4189-e35a-b23aac500c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "G4unnlo6gq-5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   num_indices  mean_num_features  mean_discrete_features  \\\n",
              "0           25               1.88                    5.04   \n",
              "\n",
              "   mean_uniform_features  mean_std_dev  mean_max_std_dev  mean_min_std_dev  \\\n",
              "0                    4.8      3.232877          4.253755          2.043039   \n",
              "\n",
              "   percentage_categorical_features  percentage_numerical_features  \\\n",
              "0                        83.959044                      16.040956   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=False_scaling=False  \\\n",
              "0                                           0.473591        \n",
              "\n",
              "   mean_spectral_weighting=False_scaling=False  \\\n",
              "0                                     0.454918   \n",
              "\n",
              "   mean_HDBSCAN_weighting=False_scaling=False  \\\n",
              "0                                    0.197757   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=True_scaling=False  \\\n",
              "0                                             0.6756       \n",
              "\n",
              "   mean_spectral_weighting=True_scaling=False  \\\n",
              "0                                    0.651994   \n",
              "\n",
              "   mean_HDBSCAN_weighting=True_scaling=False  \\\n",
              "0                                   0.175412   \n",
              "\n",
              "   mean_gower_ward_maxclust_weighting=True_scaling=True  \\\n",
              "0                                           0.482945      \n",
              "\n",
              "   mean_spectral_weighting=True_scaling=True  \\\n",
              "0                                   0.500728   \n",
              "\n",
              "   mean_HDBSCAN_weighting=True_scaling=True  mean_AgglomerativeClustering  \n",
              "0                                  0.230732                      0.484181  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8bbbd792-e00e-4380-8d68-4d3356566630\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_indices</th>\n",
              "      <th>mean_num_features</th>\n",
              "      <th>mean_discrete_features</th>\n",
              "      <th>mean_uniform_features</th>\n",
              "      <th>mean_std_dev</th>\n",
              "      <th>mean_max_std_dev</th>\n",
              "      <th>mean_min_std_dev</th>\n",
              "      <th>percentage_categorical_features</th>\n",
              "      <th>percentage_numerical_features</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=False_scaling=False</th>\n",
              "      <th>mean_spectral_weighting=False_scaling=False</th>\n",
              "      <th>mean_HDBSCAN_weighting=False_scaling=False</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=True_scaling=False</th>\n",
              "      <th>mean_spectral_weighting=True_scaling=False</th>\n",
              "      <th>mean_HDBSCAN_weighting=True_scaling=False</th>\n",
              "      <th>mean_gower_ward_maxclust_weighting=True_scaling=True</th>\n",
              "      <th>mean_spectral_weighting=True_scaling=True</th>\n",
              "      <th>mean_HDBSCAN_weighting=True_scaling=True</th>\n",
              "      <th>mean_AgglomerativeClustering</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "      <td>1.88</td>\n",
              "      <td>5.04</td>\n",
              "      <td>4.8</td>\n",
              "      <td>3.232877</td>\n",
              "      <td>4.253755</td>\n",
              "      <td>2.043039</td>\n",
              "      <td>83.959044</td>\n",
              "      <td>16.040956</td>\n",
              "      <td>0.473591</td>\n",
              "      <td>0.454918</td>\n",
              "      <td>0.197757</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.651994</td>\n",
              "      <td>0.175412</td>\n",
              "      <td>0.482945</td>\n",
              "      <td>0.500728</td>\n",
              "      <td>0.230732</td>\n",
              "      <td>0.484181</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bbbd792-e00e-4380-8d68-4d3356566630')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8bbbd792-e00e-4380-8d68-4d3356566630 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8bbbd792-e00e-4380-8d68-4d3356566630');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7VKyiq3yj4S"
      },
      "source": [
        "#Section 4: Compute time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TIGVgB_1a0G"
      },
      "source": [
        "Here the quadratic time complexity for the calculation of the Gower matrix is illustrated using fixed hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwISVY6TylNQ"
      },
      "outputs": [],
      "source": [
        "runtime_results = {}\n",
        "\n",
        "# Generate synthetic data\n",
        "n_samples = 5000\n",
        "df = create_mixed_synthetic_data(n_samples=n_samples)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['label']).values\n",
        "labels = df['label']\n",
        "\n",
        "# Calculate n_categorical from the sum of n_discrete and n_uniform\n",
        "n_features = 5\n",
        "n_discrete = 2\n",
        "n_uniform = 2\n",
        "n_categorical = n_discrete + n_uniform\n",
        "cat_vec = [False] * (n_features - n_discrete) + [True] * n_categorical\n",
        "\n",
        "# Measure time for Gower matrix computation with weighting=True\n",
        "start_time = time()\n",
        "w =  np.array(calculate_weights(df.drop(columns=['label']), cat_vec)) # Replace with your weight calculation function\n",
        "gow_weighted = gower.gower_matrix(X, cat_features=cat_vec, weight=w)\n",
        "runtime_results['With Weighting'] = time() - start_time\n",
        "\n",
        "# Measure time for Gower matrix computation with weighting=False\n",
        "start_time = time()\n",
        "gow_unweighted = gower.gower_matrix(X, cat_features=cat_vec)\n",
        "runtime_results['Without Weighting'] = time() - start_time\n",
        "\n",
        "print(\"Runtimes for different weighting options:\", runtime_results)\n",
        "\n",
        "# Initialize a dictionary to store iterative runtime results for increasing samples\n",
        "# Initialize dictionaries to store iterative runtime results for increasing samples\n",
        "iterative_runtime_results_unweighted = {}\n",
        "iterative_runtime_results_weighted = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgBI2Q1xzctZ"
      },
      "outputs": [],
      "source": [
        "# List of sample sizes to iterate over\n",
        "sample_sizes = [1000, 2000, 4000, 8000, 16000, 32000]\n",
        "\n",
        "# Iterate through varying sample sizes\n",
        "for n_samples in sample_sizes:\n",
        "    df = create_mixed_synthetic_data(n_samples=n_samples)\n",
        "    X = df.drop(columns=['label']).values\n",
        "    labels = df['label']\n",
        "\n",
        "    # Measure time for Gower matrix computation with weighting=False\n",
        "    start_time = time()\n",
        "    gow_unweighted = gower.gower_matrix(X, cat_features=cat_vec)\n",
        "    iterative_runtime_results_unweighted[n_samples] = time() - start_time\n",
        "\n",
        "    # Measure time for Gower matrix computation with weighting=True\n",
        "    start_time = time()\n",
        "    w =  np.array(calculate_weights(df.drop(columns=['label']), cat_vec))\n",
        "    gow_weighted = gower.gower_matrix(X, cat_features=cat_vec, weight=w)\n",
        "    iterative_runtime_results_weighted[n_samples] = time() - start_time\n",
        "\n",
        "print(\"Runtimes for increasing sample sizes without weighting:\", iterative_runtime_results_unweighted)\n",
        "print(\"Runtimes for increasing sample sizes with weighting:\", iterative_runtime_results_weighted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VC3nqhazgNq"
      },
      "outputs": [],
      "source": [
        "# Convert dictionary keys and values to lists for plotting\n",
        "unweighted_samples = list(iterative_runtime_results_unweighted.keys())\n",
        "unweighted_times = list(iterative_runtime_results_unweighted.values())\n",
        "\n",
        "weighted_samples = list(iterative_runtime_results_weighted.keys())\n",
        "weighted_times = list(iterative_runtime_results_weighted.values())\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(unweighted_samples, unweighted_times, marker='o', linestyle='-', label='Without Weighting')\n",
        "plt.plot(weighted_samples, weighted_times, marker='s', linestyle='-', label='With Weighting')\n",
        "plt.xlabel('Number of Samples')\n",
        "plt.ylabel('Runtime (seconds)')\n",
        "plt.title('Runtime for Computing Gower Distance Matrix')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzPCJO9i1VTW"
      },
      "source": [
        "#Section 5: Comparison on real life data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArDf7WO11o5x"
      },
      "source": [
        "Lastly the performance of the used clustering algorithms on the processed Cleveland Heart Disease Dataset is measured. For a more detailed description, look into Section 5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXeV3VzN1Ydc"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv('/content/processed.cleveland.data') # Or whereever it is saved\n",
        "data.columns=[str(i) for i in data.columns]\n",
        "labels=data[\"0\"]\n",
        "data=data.drop(columns=\"0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA_RSDp32Iyd",
        "outputId": "fcd97a50-de0e-4818-96f6-2100b357b5ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    163\n",
              "1     55\n",
              "2     36\n",
              "3     35\n",
              "4     13\n",
              "Name: 0, dtype: int64"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.value_counts() # Distribution of the target classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMzNBxbi2NaH"
      },
      "outputs": [],
      "source": [
        "cat_vec=[False, True, True, False, False, True, True, False, True, False, True, False, True] # FTTFFTTFTFTFT corresponding to categorical features\n",
        "assert len(cat_vec)==len(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_Ejifv92R0Z"
      },
      "outputs": [],
      "source": [
        "for is_cat, col in zip(cat_vec, data.columns): # Transform columns to the right type (numerical, categorical)\n",
        "    if is_cat:\n",
        "        data[col] =  data[col].astype('category')\n",
        "    else:\n",
        "         data[col] = pd.to_numeric( data[col], errors='coerce')\n",
        "\n",
        "# Handle missing values\n",
        "data.replace('?', np.nan, inplace=True)\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQjo6MR62afL",
        "outputId": "a64a204d-0ebf-4384-8078-35eba4ecee01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(296, 13)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdAVSOwq2cNb"
      },
      "outputs": [],
      "source": [
        "labels = labels.loc[data.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKjkYJltcK7f"
      },
      "outputs": [],
      "source": [
        "def gower_ari(gow, labels, linkage_method='ward', criterion='maxclust', t=0.5):\n",
        "    # Perform hierarchical clustering\n",
        "    Zd = linkage(squareform(gow), method=linkage_method)\n",
        "\n",
        "    # Form flat clusters from the hierarchical clustering defined by the linkage matrix\n",
        "    if criterion == \"maxclust\":\n",
        "        cld1 = fcluster(Zd, 5, criterion=criterion)  # Hard-coded centers as 5\n",
        "    else:\n",
        "        cld1 = fcluster(Zd, t=t, criterion=criterion)\n",
        "\n",
        "    # Compute Adjusted Rand index\n",
        "    ari = adjusted_rand_score(labels, cld1)\n",
        "\n",
        "    return ari, cld1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTNcstwpdeLe"
      },
      "outputs": [],
      "source": [
        "def cluster_data_agg(data, labels, cat_indices):\n",
        "    # Transform the data\n",
        "    transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), cat_indices)],\n",
        "                                    remainder='passthrough', sparse_threshold=0)\n",
        "    X_transformed = transformer.fit_transform(data)\n",
        "\n",
        "    # Initialize and fit AgglomerativeClustering\n",
        "    agg = AgglomerativeClustering(n_clusters=5)  # Hard-coded centers as 5\n",
        "    agg.fit(X_transformed)\n",
        "\n",
        "    # Get cluster labels\n",
        "    cluster_labels = agg.labels_\n",
        "\n",
        "    # Compute Adjusted Rand index\n",
        "    ari = adjusted_rand_score(labels, cluster_labels)\n",
        "\n",
        "    return ari, cluster_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47rtYElF922v"
      },
      "outputs": [],
      "source": [
        "def spectral_clustering2(gow=None, df=None, labels=None,centers=5):\n",
        "  clustering = SpectralClustering(n_clusters=centers, affinity='precomputed').fit(gow)\n",
        "  ari = adjusted_rand_score(clustering.labels_, labels)\n",
        "  return ari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmDcXOsM2hB3"
      },
      "outputs": [],
      "source": [
        "def compare_on_single_dataset(df, labels, cat_vec):\n",
        "    ari_scores = {}\n",
        "\n",
        "    X = df.values\n",
        "    n_features = df.shape[1]\n",
        "\n",
        "    gower_configs = [('ward', 'maxclust', False),\n",
        "                     ('ward', 'maxclust', True)]\n",
        "\n",
        "    for config in gower_configs:\n",
        "        gow = gower.gower_matrix(X, cat_features=cat_vec) if not config[2] else \\\n",
        "              gower.gower_matrix(X, cat_features=cat_vec, weight=np.array(calculate_weights(df, cat_vec)))\n",
        "        # HAC with Ward linkage\n",
        "        ari, _ = gower_ari(gow, labels)\n",
        "        method_name = f'gower_{config[0]}_{config[1]}_weighting={config[2]}'\n",
        "        ari_scores[method_name] = ari\n",
        "\n",
        "        # Spectral Clustering with similarity matrix\n",
        "        sim = 1 - gow\n",
        "        ari = spectral_clustering2(gow=sim, labels=labels)\n",
        "        method_name = f'spectral_weighting={config[2]}'\n",
        "        ari_scores[method_name] = ari\n",
        "\n",
        "        # HDBSCAN with Gower matrix\n",
        "        gow_double = gow.astype(np.float64)\n",
        "        hdbscan_cluster = hdbscan.HDBSCAN(metric='precomputed')\n",
        "        labels_pred = hdbscan_cluster.fit_predict(gow_double)#\n",
        "        print(\"Amount of data points clustered as noise for weighting=\",config[2],\": \",np.count_nonzero(labels_pred == -1)) # Print amount of data points HDBSCAN clusters as noise\n",
        "        ari = adjusted_rand_score(labels, labels_pred)\n",
        "        method_name_hdbscan = f'HDBSCAN_weighting={config[2]}'\n",
        "        ari_scores[method_name_hdbscan] = ari\n",
        "\n",
        "    # HDBSCAN baseline with one-hot encoding\n",
        "    if any(cat_vec):\n",
        "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "            X_categorical = encoder.fit_transform(X[:, np.array(cat_vec)])\n",
        "            X_numerical = X[:, ~np.array(cat_vec)]\n",
        "            X_combined = np.hstack([X_numerical, X_categorical])\n",
        "    else:\n",
        "            X_combined = X\n",
        "\n",
        "    hdbscan_cluster = hdbscan.HDBSCAN()\n",
        "    cldlabels = hdbscan_cluster.fit_predict(X_combined)\n",
        "    ari = adjusted_rand_score(labels, cldlabels)\n",
        "\n",
        "    # Storing the ARI scores for HDBSCAN applied directly to the dataset\n",
        "    ari_scores['HDBScan_baseline']=ari\n",
        "    # Agglomerative Clustering baseline with one-hot encoding\n",
        "    cat_indices = [i for i, is_cat in enumerate(cat_vec) if is_cat]\n",
        "    ari, _ = cluster_data_agg(df, labels, cat_indices)\n",
        "    ari_scores['AgglomerativeClustering'] = ari\n",
        "\n",
        "    return ari_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7j4wOBh27WE",
        "outputId": "b0f69d0e-a85f-4990-98aa-3030fbcb515c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of data points clustered as noise for weighting= False :  239\n",
            "Amount of data points clustered as noise for weighting= True :  216\n"
          ]
        }
      ],
      "source": [
        "ari_results = compare_on_single_dataset(data,labels, cat_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Uy1YNH9s3x",
        "outputId": "99255c08-217d-4c50-d9e8-2cc1eb2b23c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'gower_ward_maxclust_weighting=False': 0.14385846540519973,\n",
              " 'spectral_weighting=False': 0.10372987399375568,\n",
              " 'HDBSCAN_weighting=False': -0.03607612152284456,\n",
              " 'gower_ward_maxclust_weighting=True': 0.1949442983641996,\n",
              " 'spectral_weighting=True': 0.1403257555840935,\n",
              " 'HDBSCAN_weighting=True': -0.03677007608349537,\n",
              " 'HDBScan_baseline': 0.08622937741212872,\n",
              " 'AgglomerativeClustering': 0.0916157114444372}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ari_results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}